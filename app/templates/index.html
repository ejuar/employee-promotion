{% set image_height = "350" %}
{% macro image_c(obj) -%} 
<div class="col-xl-4 col-md-6 mb-4">
    <div class="card border-0 shadow">
      <img src="{{url_for('static', filename=obj['url'])}}" height= {{image_height}} class="card-img-top" alt="...">
      <div class="card-body text-center">
        <h5 class="card-title mb-0">{{obj['name']}}</h5>
        <div class="card-text text-black-50">{{obj['role']}}</div>
      </div>
    </div>
  </div>
{%- endmacro %}
{% set contributors = [
    {'url': 'images/ej_img.png', 'name': 'Eric', 'role': 'Instructor'},
    {'url': 'images/alex_duffy_img.png', 'name': 'Alex', 'role': 'Instructor'},
    {'url': 'images/mitchell_img.png', 'name': 'Mitch', 'role': 'Instructor'},
] %}
<!DOCTYPE html>
<html lang="en">
    <head>
        <meta charset="utf-8" />
        <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no" />
        <meta name="description" content="" />
        <meta name="author" content="" />
        <title>Predicting employee promotion using HR data</title>
        <link rel="icon" type="image/x-icon" href="{{url_for('static', filename='assets/favicon.ico')}}" />
        <!-- Core theme CSS (includes Bootstrap)-->
        <link href="{{url_for('static', filename='css/styles.css')}}" rel="stylesheet" />
    </head>
    <body id="page-top">
        <!-- Navigation-->
        <nav class="navbar navbar-expand-lg navbar-dark bg-dark fixed-top" id="mainNav">
            <div class="container px-4">
                <a class="navbar-brand" href="#page-top">Employee Promotion Predictor</a>
                <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarResponsive" aria-controls="navbarResponsive" aria-expanded="false" aria-label="Toggle navigation"><span class="navbar-toggler-icon"></span></button>
                <div class="collapse navbar-collapse" id="navbarResponsive">
                    <ul class="navbar-nav ms-auto">
                        <li class="nav-item"><a class="nav-link" href="#about">About</a></li>
                        <li class="nav-item"><a class="nav-link" href="#data-exploration">Data Exploration</a></li>
                        <li class="nav-item"><a class="nav-link" href="#models">Machine learning</a></li>
                        <li class="nav-item"><a class="nav-link" href="#evaluation">Evaluation</a></li>
                        <li class="nav-item"><a class = "nav-link" href="#conclusion">Conclusion</a></li>
                        <li class="nav-item"><a class="nav-link" href="#team">Team</a></li>
                    </ul>
                </div>
            </div>
        </nav>
        <!-- Header-->
        <header class="bg-primary bg-gradient text-white">
            <div class="container px-4 text-center">
                <h1 class="fw-bolder">Predicting employee promotion using HR data</h1>
                <p class="lead">A data science project by AI Camp instructors</p>
                <a class="btn btn-lg btn-light" href="#about">See our work below</a>
            </div>
        </header>
        <!-- About section-->
        <section id="about">
            <div class="container px-4">
                <div class="row gx-4 justify-content-center">
                    <div class="col-lg-8">
                        <h2>Project Objective</h2>
                        <p class="lead">
                            Direct labor costs are an important factor considered by managerial accountants when balancing budgets in a business. 
                            Predicting the employees that will be promoted within a given year will aid in predicting future department budget allocations. 
                            Given a company's employee information, classification models can be used to predict the employees that will be promoted in a given year.
                        </p>
                    </div>
                </div>
            </div>
        </section>
        <!-- Data-exploration section-->
        <section class="bg-light" id="data-exploration">
            <div class="container px-4">
                <div class="row gx-4 justify-content-center">
                    <div class="col-lg-8">
                    <h1 id="top-level-heading">Data Exploration</h1>
                    <br>
                    </div>
                    <div class="col-lg-8">
                        <h3>HR dataset characteristics</h3>
                        <p class="lead">
                            The dataset used is an anonymous company's HR data in one year obtained from Kaggle. 
                            The data is tabular with a size of ~50,000 rows and 14 columns. 
                            Each row describes an employee with column features such as “is_promoted,” “age,” “education,” “department,” etc.. 
                            The target feature, “is_promoted”, has two values that represent if an employee was promoted that year.
                        </p>
                        <br><br>
                    </div>                    
                    <div class="col-lg-8">
                        <h3>Univariate Distribution of Features</h3>
                        <p class="lead">
                            The features in the dataset were separated into respective variable types, numerical or categorical. 
                            The distributions of numerical data were analyzed with histograms, and the distributions of categorical data were analyzed with bar charts. 
                            The feature "employee_id" was removed since unique identifiers do not influence employee promotions.
                        </p>
                        <iframe width="720" height="570" title="Numerical feature distributions" src="{{url_for('static', filename='common/numerical_histogram.html')}}"></iframe> 
                        <iframe width="908" height="720" title="Categorical feature distributions" src="{{url_for('static', filename='common/categorical_barchart.html')}}"></iframe> 
                        <p class="lead">
                            The target feature, “is_promoted”, is imbalanced because job promotions are generally selective; 
                            therefore, the null accuracy (accuracy obtained when only predicting the most common label) will be very high for this dataset. 
                            The imbalanced data may result in models not accurately predicting instances of the minority target value.
                        </p>
                        <br><br>
                    </div>

                    <div class="col-lg-8">
                        <h3>Bivariate Distribution of Features</h3>
                        <p class="lead">
                            Trends in promotions may become immediately apparent with bivariate visualizations. 
                            Boxplots were used to analyze the distribution of numerical features and promotion status. 
                            Stacked bar charts were used to analyze the proportions of categorical features that are promoted.
                        </p>
                        <iframe width="720" height="570" title="Numerical features vs promotion" src="{{url_for('static', filename='common/numerical_promotion_boxplot.html')}}"></iframe>
                        <p class="lead">
                            These figures show that the number of trainings, age, and length of service do not individually correlate with promotion among employees. 
                            Out of the numerical features, it appears that "Avg. Training Score" has different mean values for each "promoted" value; 
                            employees that were promoted tend to have a higher average training score.
                        </p>
                        <iframe width="908" height="720" title="Categorical features vs promotion" src="{{url_for('static', filename='common/categorical_promotion_barchart.html')}}"></iframe>  
                        <p class="lead">
                            There are noticeable differences in promotion among the "KPIs Met >80%", "Awards Won?", and "Previous Year Rating" features.
                        </p>
                        <br><br>              
                    </div>       
                    <div class="col-lg-8">
                        <h3>Correlation Heatmap</h3>
                        <p class="lead">
                            The correlations between numerical features and binary features were analyzed with a correlation matrix heatmap. 
                            It is important to remove an instance of very highly correlated data to prevent bias. 
                            In this dataset, no numerical features or binary features were removed because there were no instances of very high absolute correlations.
                        </p>
                        <iframe width="908" height="720" title="Correlation matrix" src="{{url_for('static', filename='common/correlation_heatmap.html')}}"></iframe> 
                        <br><br>
                    </div>
                    <div class="col-lg-8">
                        <h3>Categorical Feature and Target Feature Dependence</h3>
                        <p class="lead">
                            A Chi-Square test of independence is used to find the relationship between the input features and output feature.
                            The Chi-Square test of independence assumes the null hypothesis "each input feature and target feature are independent of each other."
                            If the p-value is less than the decided significance value, the null hypothesis is rejected; 
                            there is a statistically significant association between the input feature and target feature.
                            It was found that removing categorical features with p-values greater than 0.2 improved model performances the most.
                        </p>
                        <iframe width="908" height="520" title="Correlation matrix" src="{{url_for('static', filename='common/categorical_pvalues.html')}}"></iframe> 
                    </div>                                              
                </div>
            </div>
        </section>
        <!-- Models section-->
        <section id="models">
            <div class="container px-4">
                <div class="row gx-4 justify-content-center">
                    <div class="col-lg-8">
                        <h1>Machine Learning</h1></br>
                    </div>

                    <div class="col-lg-8">
                        <h2>Logistic Regression</h2>
                        <p class="lead">
                            Logistic regression is an algorithm for binary classification that uses the sigmoid equation.
                            The possible outputs of the logistic regression equation lie between 0 and 1, and a threshold of 0.5 is used to decide the classification of the outputs; 
                            values greater than 0.5 are classified as 1, while values less than or equal to 0.5 are classified as 0.
                            The possible solvers for logistic regression in Sklearn use a form of optimization with gradients.
                        </p>
                        <img width="500" height="400" src="{{url_for('static', filename='images/sigmoid.png')}}" class="page-image" /> 
                    </div>
                    <div class="col-lg-8">
                        <h2>Stochastic Gradient Descent Classifier</h2>
                        <p class="lead">
                            Sklearn's SGDClassifier uses stochastic gradient descent with different possible loss functions, like log-loss or squared error.
                            Sklearn generally uses batch gradient descent for optimizing parameters. 
                            Batch gradient descent is optimization using the entire dataset at a time. 
                            In contrast, stochastic gradient descent uses one sample at a time when training a model. 
                            SGD is less computer memory intensive since only one sample is trained at a time. 
                            The training steps in batch gradient descent are less influenced by noise because it is an average of all the gradients in the dataset; 
                            therefore, the training steps usually move in a predictable direction. 
                            However, the training steps in SGD do not move in a predictable direction because each sample is noisy with respect to other samples. 
                            SGD can be more accurate than batch gradient descent when the error manifold has many local maxima/minima.
                            A balance between SGD and batch gradient descent is mini-batch gradient descent.
                            Mini-batch gradient descent uses small sample subsets at a time when training.                        
                        </p>
                        <img width="600" height="300" src="{{url_for('static', filename='images/gradient_descent.png')}}" class="page-image" />
                    </div>
                    <div class="col-lg-8">
                        <h2>XGBoost Classifier</h2>
                        <p class="lead">
                            The XGBoost Classifier is a type of gradient boosted decision tree algorithm designed to be fast and accurate. 
                            Gradient boosted decision trees are decision trees with ensemble boosting. 
                            Ensemble boosting involves using many weak learners sequentially, such that the outcome of one learner influences the parameters of the next learner. 
                            The overall steps for gradient boosted decision trees are:
                        </p>
                        <ol class="lead" type="1">
                            <li>Set a single prediction for the entire dataset.</li>
                            <li>Calculate the residuals of the predictions.</li>
                            <li>Create a decision tree that predicts residuals.</li>
                            <li>Make a prediction of the entire dataset using each new tree and the initial staring prediction.</li>
                            <li>Repeat steps 2 to 4 until an accuracy threshold or some stopping threshold is met.</li>
                        </ol>
                        <img width="650" height="300" src="{{url_for('static', filename='images/xgboost.png')}}" class="page-image" />
                        <p class="lead">
                            Some major differences between XGBoost and gradient boosted decision trees are:
                        </p>
                        <ol class="lead" type="1">
                            <li>XGBoost uses a more efficient method to create decision trees.</li>
                            <li>XGBoost uses a regularization term to lower model overfitting.</li>
                            <li>XGBoost uses first and second derivatives in optimization.</li>
                            <li>XGBoost improves the efficiency of computational resource allocation.</li>
                        </ol>
                    </div>                                        
                    <div class="col-lg-8">
                        <h2>Light Gradient Boosted Machine Classifier</h2>
                        <p class="lead">
                            The LightGBM Classifier is also a gradient boosted decision tree algorithm. 
                            LightGBM usually performs faster than XGBoost and can have similar accuracy. 
                            LightGBM also uses a regularization term, both the first and second derivatives in optimization, and efficient use of computational resources.
                        </p>

                        <p class="lead">
                            Some major differences between LightGBM and XGBoost are:
                        </p>
                        <ol class="lead" type="1">
                            <li>
                                XGBoost uses level-wise tree growth, while LightGBM uses leaf-wise tree growth. 
                                Leaf-wise tree growth is faster because it does not depend on levels being filled. 
                                However, leaf-wise tree growth can lead to overfitting.
                            </li>
                            <li>
                                LightGBM uses gradient based one side sampling (GOSS) which downsamples the gradients used to update the decision trees. 
                                In GOSS, rather than using all the gradients, only a small portion of the largest gradients are sampled with a random small sample of lower gradients. 
                                This improves the training speed.
                            </li>
                            <li>
                                LightGBM uses exclusive feature bundling (EFB) which combines exclusive features into one feature. 
                                The motivation behind combining multiple features is that sparse matrices often have mutual features and can be reduced to a lower dimension to improve efficiency.
                            </li>
                        </ol>
                        <img width="400" height="200" src="{{url_for('static', filename='images/level_wise_tree.png')}}" class="page-image" />
                        <br>
                        <img width="450" height="200" src="{{url_for('static', filename='images/leaf_wise_tree.png')}}" class="page-image" />                        
                    </div>
                </div>
            </div>
        </section>
        <!-- Evaluation section-->
        <section class="bg-light" id="evaluation">
            <div class="container px-4">
                <div class="row gx-4 justify-content-center">
                    <div class="col-lg-8">
                        <h1>Model Evaluation</h1></br>
                    </div>
                    <div class="col-lg-8">
                        <h2>Confusion Matrics</h2>
                        <p class="lead">
                            For binary classification, confusion matrices display model outputs in four distinct categories: 
                            true positive (bottom right), true negative (top left), false positive (top right), and false negative (bottom left).
                            True positive and true negative represent samples the model correctly identified as positive or negative.
                            False positive and false negative represent samples the model incorrectly identified as positive or negative.
                        </p>
                        <iframe width="908" height="720" title="Confusion Matrix" src="{{url_for('static', filename='common/confusion_matrices.html')}}"></iframe>  
                        <p class="lead">
                            All models showed relatively similar performance, having almost the same magnitude of predictions in each confusion matrix category.
                            A perfect model would have produced a confusion matrix with 8859 samples predicted in the true negative category, 
                            852 samples predicted in the true positive category, and 0 samples predicted in the false positive and false negative categories.
                        </p>                    
                    </div>
                    <div class="col-lg-8">
                        <h2>Classification Reports</h2>
                        <p class="lead">
                            The classification reports include metrics such as precision, recall, and f1-score which are calculated from the model confusion matrices.
                            Precision measures the proportion of samples that are actually positive out of all the samples predicted positive.
                            Recall measures the proportion of samples that are predicted positive out of all the samples actually positive.
                            The f1-score describes the overall performance of a target class by calculating the harmonic mean of precision and recall.
                        </p>
                        <img width="908" height="400" src="{{url_for('static', filename='images/classification_reports.png')}}" class="page-image" />
                        <p class="lead">
                            The order of model performance from best to worst according to average f1-score is LightGBM, XGBoost, SGD, then Logistic Regression.
                        </p>  
                    </div>       
                    <div class="col-lg-8">
                        <h2>ROC and Precision-Recall Curves</h2>
                        <p class="lead">
                            The precision-recall curve plots the relationship between a model's precision and recall.
                            For an imbalanced binary dataset with a positive minority class, precision-recall values that are above the complement of the null accuracy represent improved model performance.
                            The area under the curve, or AUC, provides an aggregate measure of performance.
                            Generally, the closer AUC is closer to 1, the better the model performance.
                            Precision-recall curves may be more appropriate in describing the performance of a model with an imbalanced dataset because it ignores an imbalanced class.
                        </p>
                        <iframe width="908" height="520" title="ROC PRC" src="{{url_for('static', filename='common/roc_prc.html')}}"></iframe>  
                        <p class="lead">
                            There are clear distinctions between the performance of the models. 
                            Logistic regression and SGD performed similarly, most likely because the SGD used a log-loss cost function and did logistic regression.
                            LightGBM and XGBoost also performed similarly because they are similar algorithms.
                            The ROC Curves may be misleading because the AUC's are about 0.83 to 0.91 which would suggest excellent model performances.
                            However, precision-recall curves may be more appropriate in describing the performance of a model with an imbalanced dataset because it ignores the imbalanced negative class. 
                            The precision-recall curves show that the models have poor recall.
                        </p>
                    </div>                                                   
                </div>
            </div>
        </section>
        <!-- Conclusion section-->
        <section id="conclusion">
            <div class="container px-4">
                <div class="row gx-4 justify-content-center">
                    <div class="col-lg-8">
                        <h1 id="top-level-heading">Conclusion</h1>
                    </div>
                    <div class="col-lg-8">
                        <p class="lead">
                            Each tuned model had an accuracy higher than the null accuracy; 
                            therefore, it is better to use a model than not use a model. 
                            The best performing algorithms were the LightGBM Classifier and XGBoost Classifier, 
                            which is not very surprising since these algorithms have been used many frequently by top placing Kaggle competitors.
                            Overall, each model had poor recall, which is likely due to this dataset being highly imbalanced.
                            <br>
                            According to correlation tests and chi-square tests of independence, the most important features in predicting promotion are 
                            average training scores, winning an award, having KPIs met above 80%, previous year rating, being referred, being part of certain departments, and being from certain regions.
                        </p>                            
                    </div>
                    <div class="col-lg-8">
                        <h1 id="top-level-heading">Future Directions</h1>
                    </div>
                    <div class="col-lg-8">
                        <p class="lead">
                            While resampling techniques were tested to correct the imbalance of data, no resampling technique improved the model performance. 
                            It is possible a more exhaustive search for resampling techniques may improve classification of the imbalanced data. 
                            Missing values in the data were handled with mode imputation. 
                            However, dealing with missing data may be improved by using a clustering method or even just creating a new class for missing data. 
                            The XGBoost algorithm could have been tuned more, but it requires more computational resources to tune more paramaters.
                            Another possible improvement may be using binary encoding for the "region" feature. 
                            By using one-hot encoding for the "region" feature, high sparsity is introduced to the dataset. 
                            LightGBM will be less impacted from binary encoding because it uses exclusive feature bundling, but the other classification models may improve.
                        </p>                            
                    </div>
                </div>                    
                </div>
            </div>
        </section>

        <!-- Team section-->
        <section class="bg-light" id="team">
        <header class="bg-primary text-center py-5 mb-4">
            <div class="container px-4">
              <h1 class="fw-light text-white">Meet the Extraordinary AI Camp Team</h1>
            </div>
        </header>
            <div class="container px-4">
                <div class="row">
                    {% for contributor in contributors %} 
                        {{ image_c(contributor) }}
                    {% endfor %}
                </div>
            </div>
        </section>
        <!-- Footer-->
        <footer class="py-5 bg-dark">
            <div class="container px-4"><p class="m-0 text-center text-white">Copyright &copy; AI Camp 2022</p></div>
        </footer>
        <!-- Bootstrap core JS-->
        <script src="https://cdn.jsdelivr.net/npm/bootstrap@5.1.3/dist/js/bootstrap.bundle.min.js"></script>
        <!-- Core theme JS-->
        <script src="/static/js/scripts.js"></script>
    </body>
</html>